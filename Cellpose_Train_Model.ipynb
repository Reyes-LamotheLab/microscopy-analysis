{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7c7V4yEqDc_"
      },
      "source": [
        "# Running cellpose 2.0 in colab with a GPU\n",
        "# TRAINING MODEL ONLY\n",
        "\n",
        "<font size = 4>Cellpose 2.0 now allows you to train your own models in the GUI!\n",
        "\n",
        "In this notebook, you can **train** a custom model using your labels (`_seg.npy`) files, or other labels as `_masks.tif` files. If you already have a trained model go to the other notebook **run custom model**\n",
        "\n",
        "For more details on cellpose 2.0 check out the [paper](https://www.biorxiv.org/content/10.1101/2022.04.01.486764v1) or the [talk](https://www.youtube.com/watch?v=3ydtAhfq6H0).\n",
        "\n",
        "Mount your google drive to access all your image files, segmentations, and custom models. This also ensures that any models you train are saved to your google drive.\n",
        "\n",
        "This notebook was inspired by the Zero-Cost Deep-Learning to Enhance Microscopy project (https://github.com/HenriquesLab/DeepLearning_Collab/wiki). Jointly developed by the Jacquemet (link to https://cellmig.org/) and Henriques (https://henriqueslab.github.io/) laboratories. Please check out their great work!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvyuR08OZfw4"
      },
      "source": [
        "# Setup\n",
        "\n",
        "We will first install cellpose 2.0, check the GPU is working, and mount google drive to get your models and images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbqFni8kuFar"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUfSFlZgp1aV"
      },
      "source": [
        "Install cellpose -- by default the torch GPU version is installed in COLAB notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlMnqge-lQ9s",
        "outputId": "00fddf9a-b965-4a52-e4d5-ccf822f6996e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python-headless<4.3 in c:\\users\\jpras\\.conda\\envs\\cellpose\\lib\\site-packages (4.2.0.34)\n",
            "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\jpras\\.conda\\envs\\cellpose\\lib\\site-packages (from opencv-python-headless<4.3) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"opencv-python-headless<4.3\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2cBEO1PLuO7"
      },
      "source": [
        "Check CUDA version and that GPU is working in cellpose and import other libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt8hgC7rniP8",
        "outputId": "4027460e-78ea-449c-80bc-7a1b79159889"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'nvcc' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Jul 21 14:16:06 2023       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 531.79                 Driver Version: 531.79       CUDA Version: 12.1     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA GeForce RTX 3060 Ti    WDDM | 00000000:01:00.0  On |                  N/A |\n",
            "|  0%   35C    P8               12W / 225W|    873MiB /  8192MiB |     11%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|    0   N/A  N/A      3496    C+G   ...on\\114.0.1823.82\\msedgewebview2.exe    N/A      |\n",
            "|    0   N/A  N/A      5340    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
            "|    0   N/A  N/A      5864    C+G   ...Desktop\\app-3.2.3\\GitHubDesktop.exe    N/A      |\n",
            "|    0   N/A  N/A      6000    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
            "|    0   N/A  N/A      6428    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
            "|    0   N/A  N/A      7284    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
            "|    0   N/A  N/A      7840    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
            "|    0   N/A  N/A      8980    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
            "|    0   N/A  N/A      9736    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
            "|    0   N/A  N/A      9964    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
            "|    0   N/A  N/A     10728    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
            "|    0   N/A  N/A     10944    C+G   ...__8wekyb3d8bbwe\\WindowsTerminal.exe    N/A      |\n",
            "|    0   N/A  N/A     11804      C   ...Ras\\.conda\\envs\\cellpose\\python.exe    N/A      |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "2023-07-21 14:16:06,494 [INFO] ** TORCH CUDA version installed and working. **\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi\n",
        "\n",
        "import os, shutil\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from cellpose import core, utils, io, models, metrics\n",
        "from glob import glob\n",
        "use_GPU = core.use_gpu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfE75htF0l84"
      },
      "source": [
        "# Train model on manual annotations\n",
        "\n",
        "Skip this if you already have a pretrained model.\n",
        "\n",
        "## Fill out the form below with the paths to your data and the parameters to start training.\n",
        "\n",
        "<font size=4>**`You do not need to change any parameter. Just the path of the training data and the model name`**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLdKNWQ4jxy5"
      },
      "source": [
        "## Training parameters\n",
        "\n",
        "<font size = 4> **Paths for training, predictions and results**\n",
        "\n",
        "\n",
        "<font size = 4>**`train_dir:`, `test_dir`:** These are the paths to your folders train_dir (with images and masks of training images) and test_dir (with images and masks of test images). You can leave the test_dir blank, but it's recommended to have some test images to check the model's performance. To find the paths of the folders containing the respective datasets, go to your Files on the left of the notebook, navigate to the folder containing your files and copy the path by right-clicking on the folder, **Copy path** and pasting it into the right box below.\n",
        "\n",
        "<font size = 4>**`initial_model`:** Choose a model from the cellpose [model zoo](https://cellpose.readthedocs.io/en/latest/models.html#model-zoo) to start from.\n",
        "\n",
        "<font size = 4>**`model_name`**: Enter the path where your model will be saved once trained (for instance your result folder).\n",
        "\n",
        "<font size = 4>**Training parameters**\n",
        "\n",
        "<font size = 4>**`number_of_epochs`:** Input how many epochs (rounds) the network will be trained. At least 100 epochs are recommended, but sometimes 250 epochs are necessary, particularly from scratch. **Default value: 100**\n",
        "\n",
        "## OUTPUT\n",
        "<font size =4>**`The model will be saved in the model folder which will be in the training folder of your images`**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQI4aUxCjz3n",
        "outputId": "9a7e2d03-a460-44d5-bb93-2d45148fdf31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Default advanced parameters enabled\n"
          ]
        }
      ],
      "source": [
        "#@markdown ###Path to images and masks:\n",
        "\n",
        "train_dir = input(\"Enter the path of the folder that contains the training data (image file + _seg.npy file)\") #@param {type:\"string\"}\n",
        "train_dir=train_dir.replace('\\\\', '/')\n",
        "\n",
        "# This is facultative\n",
        "test_dir = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#Define where the patch file will be saved\n",
        "base = \"/content\"\n",
        "\n",
        "# model name and path\n",
        "#@markdown ###Name of the pretrained model to start from and new model name:\n",
        "from cellpose import models\n",
        "initial_model = \"cyto\" #@param ['cyto','nuclei','tissuenet','livecell','cyto2','CP','CPx','TN1','TN2','TN3','LC1','LC2','LC3','LC4','scratch']\n",
        "\n",
        "#\n",
        "model_name = input(\"What will be the name of the model/model file\")   #@param {type:\"string\"}\n",
        "\n",
        "# other parameters for training.\n",
        "#@markdown ###Training Parameters:\n",
        "#@markdown Number of epochs:\n",
        "n_epochs =  100#@param {type:\"number\"}\n",
        "\n",
        "Channel_to_use_for_training = \"Grayscale\" #@param [\"Grayscale\", \"Blue\", \"Green\", \"Red\"]\n",
        "\n",
        "# @markdown ###If you have a secondary channel that can be used for training, for instance nuclei, choose it here:\n",
        "\n",
        "Second_training_channel= \"None\" #@param [\"None\", \"Blue\", \"Green\", \"Red\"]\n",
        "\n",
        "\n",
        "#@markdown ###Advanced Parameters\n",
        "\n",
        "Use_Default_Advanced_Parameters = True #@param {type:\"boolean\"}\n",
        "#@markdown ###If not, please input:\n",
        "learning_rate = 0.1 #@param {type:\"number\"}\n",
        "weight_decay = 0.0001 #@param {type:\"number\"}\n",
        "\n",
        "if (Use_Default_Advanced_Parameters):\n",
        "  print(\"Default advanced parameters enabled\")\n",
        "  learning_rate = 0.1\n",
        "  weight_decay = 0.0001\n",
        "\n",
        "#here we check that no model with the same name already exist, if so delete\n",
        "model_path = train_dir + 'models/'\n",
        "if os.path.exists(model_path+'/'+model_name):\n",
        "  print(\"!! WARNING: \"+model_name+\" already exists and will be deleted in the following cell !!\")\n",
        "\n",
        "if len(test_dir) == 0:\n",
        "  test_dir = None\n",
        "\n",
        "# Here we match the channel to number\n",
        "if Channel_to_use_for_training == \"Grayscale\":\n",
        "  chan = 0\n",
        "elif Channel_to_use_for_training == \"Blue\":\n",
        "  chan = 3\n",
        "elif Channel_to_use_for_training == \"Green\":\n",
        "  chan = 2\n",
        "elif Channel_to_use_for_training == \"Red\":\n",
        "  chan = 1\n",
        "\n",
        "\n",
        "if Second_training_channel == \"Blue\":\n",
        "  chan2 = 3\n",
        "elif Second_training_channel == \"Green\":\n",
        "  chan2 = 2\n",
        "elif Second_training_channel == \"Red\":\n",
        "  chan2 = 1\n",
        "elif Second_training_channel == \"None\":\n",
        "  chan2 = 0\n",
        "\n",
        "if initial_model=='scratch':\n",
        "  initial_model = 'None'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JRxBPmatrK7"
      },
      "source": [
        "## Train new model\n",
        "\n",
        "Using settings from form above, train model in notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcYskYudMajM",
        "outputId": "55158647-8577-4eed-a463-4f151c428127"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:/Users/JpRas/Downloads/train+model_Viri-20230718T174006Z-001/train+model_Viri/train\n",
            "creating new log file\n",
            "2023-07-21 14:16:10,910 [INFO] WRITING LOG OUTPUT TO C:\\Users\\JpRas\\.cellpose\\run.log\n",
            "2023-07-21 14:16:10,910 [INFO] \n",
            "cellpose version: \t2.2.1 \n",
            "platform:       \twin32 \n",
            "python version: \t3.8.17 \n",
            "torch version:  \t2.0.1\n",
            "2023-07-21 14:16:10,911 [INFO] >> cyto << model set to be used\n",
            "2023-07-21 14:16:10,912 [INFO] ** TORCH CUDA version installed and working. **\n",
            "2023-07-21 14:16:10,913 [INFO] >>>> using GPU\n",
            "2023-07-21 14:16:11,066 [INFO] >>>> model diam_mean =  30.000 (ROIs rescaled to this size during training)\n",
            "2023-07-21 14:16:11,068 [INFO] not all flows are present, running flow generation for all images\n",
            "2023-07-21 14:16:11,271 [INFO] 6 / 6 images in C:/Users/JpRas/Downloads/train+model_Viri-20230718T174006Z-001/train+model_Viri/train folder have labels\n",
            "2023-07-21 14:16:11,451 [INFO] computing flows for labels\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00, 12.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-07-21 14:16:12,035 [INFO] >>>> median diameter set to = 30\n",
            "2023-07-21 14:16:12,035 [INFO] >>>> mean of training label mask diameters (saved to model) 26.061\n",
            "2023-07-21 14:16:12,038 [INFO] >>>> training network with 2 channel input <<<<\n",
            "2023-07-21 14:16:12,039 [INFO] >>>> LR: 0.10000, batch_size: 8, weight_decay: 0.00010\n",
            "2023-07-21 14:16:12,039 [INFO] >>>> ntrain = 6\n",
            "2023-07-21 14:16:12,040 [INFO] >>>> nimg_per_epoch = 8\n",
            "2023-07-21 14:16:17,028 [INFO] Epoch 0, Time  5.0s, Loss 2.5009, LR 0.0000\n",
            "2023-07-21 14:16:17,214 [INFO] saving network parameters to C:/Users/JpRas/Downloads/train+model_Viri-20230718T174006Z-001/train+model_Viri/train\\models/test\n",
            "2023-07-21 14:16:18,002 [INFO] Epoch 5, Time  6.0s, Loss 1.0560, LR 0.0556\n",
            "2023-07-21 14:16:18,955 [INFO] Epoch 10, Time  6.9s, Loss 0.1948, LR 0.1000\n",
            "2023-07-21 14:16:20,828 [INFO] Epoch 20, Time  8.8s, Loss 0.3175, LR 0.1000\n",
            "2023-07-21 14:16:22,711 [INFO] Epoch 30, Time 10.7s, Loss 0.2148, LR 0.1000\n",
            "2023-07-21 14:16:24,576 [INFO] Epoch 40, Time 12.5s, Loss 0.1669, LR 0.1000\n",
            "2023-07-21 14:16:26,450 [INFO] Epoch 50, Time 14.4s, Loss 0.1406, LR 0.1000\n",
            "2023-07-21 14:16:28,316 [INFO] Epoch 60, Time 16.3s, Loss 0.1323, LR 0.1000\n"
          ]
        }
      ],
      "source": [
        "# start logger (to see training across epochs)\n",
        "print(train_dir)\n",
        "logger = io.logger_setup()\n",
        "\n",
        "# DEFINE CELLPOSE MODEL (without size model)\n",
        "model = models.CellposeModel(gpu=use_GPU, model_type=initial_model)\n",
        "\n",
        "# set channels\n",
        "channels = [chan, chan2]\n",
        "\n",
        "# get files\n",
        "output = io.load_train_test_data(train_dir, test_dir, mask_filter='_seg.npy')\n",
        "train_data, train_labels, _, test_data, test_labels, _ = output\n",
        "\n",
        "new_model_path = model.train(train_data, train_labels,\n",
        "                              test_data=test_data,\n",
        "                              test_labels=test_labels,\n",
        "                              channels=channels,\n",
        "                              save_path=train_dir,\n",
        "                              n_epochs=n_epochs,\n",
        "                              learning_rate=learning_rate,\n",
        "                              weight_decay=weight_decay,\n",
        "                              nimg_per_epoch=8,\n",
        "                              model_name=model_name)\n",
        "\n",
        "# diameter of labels in training images\n",
        "diam_labels = model.diam_labels.copy()\n",
        "\n",
        "##executed in 1 min for 6 images"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
